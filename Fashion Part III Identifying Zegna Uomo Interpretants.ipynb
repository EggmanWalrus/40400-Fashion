{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion (MACS 40400: Computation and the Identification of Cultural Patterns)\n",
    "\n",
    "## Part III: Identifying Zegna Uomo Interpretants\n",
    "\n",
    "Today, we're going to attempt to discover if there are any conventional interpretants for Zegna's Uomo cologne. In class on Tuesday, we saw that the product was intended to produce interpretants related to the concepts of masculinity, mastery and sophistication. Today, we are evaluating whether these are the sorts of interpretants that are actually produced in the circles of interpreters outside of individual advertisement that we watched by analyzing the words used in YouTube videos and the things that appear in images related to the Zegna brand more generally on Flickr.\n",
    "\n",
    "In Part II, we explored meanings associated with the Zegna brand by identifying some of the most frequently appearing elements in photos associated with the brand on Flickr. The Uomo fragrance draws some of its meaning from the Zegna brand more generally and we could see that the Zegna brand does indeed seem to be associated with a certain \"urban gentleman\" masculinity.\n",
    "\n",
    "In Part III, we will explore the interpretants produced by YouTube influencers who have posted about the Uomo product in particular, using the YouTube text data gathered in Part II."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************\n",
    "\n",
    "Before class, please download the \"gensim\" package and run the following code cell to download several items for NLTK (Natural Language Toolkit). You can do this by clicking on your 'Anaconda Navigator' Program and completing the [following instructions](https://stackoverflow.com/questions/39299726/cant-find-package-on-anaconda-navigator-what-to-do-next), or by accessing your terminal and typing `conda install gensim` (and completing the installation process).\n",
    "\n",
    "If you would like further practice with NLTK and Gensim after completing this demo, check out DataCamp's [Introduction to Natural Language Processing in Python](https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python) course (especially chapters 1 and 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon Clindaniel\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jon\n",
      "[nltk_data]     Clindaniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jon Clindaniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to C:\\Users\\Jon\n",
      "[nltk_data]     Clindaniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jon\n",
      "[nltk_data]     Clindaniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jon\n",
      "[nltk_data]     Clindaniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Jon\n",
      "[nltk_data]     Clindaniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora, models\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Read in Youtube Data for Uomo from Part II\n",
    "uomo_yt_df = pd.read_json('uomo_yt100.json')\n",
    "uomo_yt_df.dropna(inplace=True)\n",
    "\n",
    "# Functions for Analysis:\n",
    "def pos_tag(text):\n",
    "    import string\n",
    "    \n",
    "    # Define stop words--common words like \"the\"--and punctuation to drop from analysis\n",
    "    stop = nltk.corpus.stopwords.words('english') + list(string.punctuation) + [\"amp\", \"39\", \"subscribe\", \"follow\",\n",
    "                                                                                \"link\", \"ermenegildo\", \"zegna\", \"uomo\",\n",
    "                                                                                \"music\", \"applause\", \"um\", \"facebook\"\n",
    "                                                                               ]\n",
    "    \n",
    "    # Tokenize words using nltk.word_tokenize, keeping only those tokens that do not appear in the stop words we defined\n",
    "    tokens = [i for i in nltk.word_tokenize(text.lower()) if i not in stop]\n",
    "    \n",
    "    # Label parts of speech automatically using NLTK\n",
    "    pos_tagged = nltk.pos_tag(tokens)\n",
    "    return pos_tagged\n",
    "\n",
    "def plot_top_adj(series, data_description):\n",
    "    # Apply part of Speech tagger that we wrote above to any Pandas series that pass into the function\n",
    "    pos_tagged = series.apply(pos_tag)\n",
    "\n",
    "    # Extend list so that it contains all words/parts of speech for all the captions\n",
    "    pos_tagged_full = []\n",
    "    for i in pos_tagged:\n",
    "        pos_tagged_full.extend(i)\n",
    "    \n",
    "    # Create Frequency Distribution of different adjectives and plot the distribution\n",
    "    fd = nltk.FreqDist(word + \"/\" + tag for (word, tag) in pos_tagged_full if tag[:2] == 'JJ')\n",
    "    fd.plot(15, title='Top 15 Adjectives for ' + data_description);\n",
    "    return\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "def get_lemmas(text):\n",
    "    import string\n",
    "    stop = nltk.corpus.stopwords.words('english') + list(string.punctuation) + [\"amp\", \"39\", \"subscribe\", \"follow\",\n",
    "                                                                                \"link\", \"ermenegildo\", \"zegna\", \"uomo\",\n",
    "                                                                                \"music\", \"applause\", \"um\", \"facebook\"\n",
    "                                                                               ]\n",
    "    tokens = [i for i in nltk.word_tokenize(text.lower()) if i not in stop]\n",
    "    lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(t, get_wordnet_pos(t)) for t in tokens]\n",
    "    return lemmas\n",
    "\n",
    "def plot_top_lemmas(series, data_description):\n",
    "    lemmas = series.apply(get_lemmas)\n",
    "\n",
    "    # Extend list so that it contains all words/parts of speech for all the captions\n",
    "    lemmas_full = []\n",
    "    for i in lemmas:\n",
    "        lemmas_full.extend(i)\n",
    "\n",
    "    nltk.FreqDist(lemmas_full).plot(20, title='Top 10 Lemmas Overall for ' + data_description);\n",
    "    return\n",
    "\n",
    "def plot_top_tfidf(series, data_description):\n",
    "    # Apply 'get lemmas' function to any Pandas Series that we pass in to get lemmas for each row in the Series\n",
    "    lemmas = series.apply(get_lemmas)\n",
    "    \n",
    "    # Initialize Series of lemmas as Gensim Dictionary for further processing\n",
    "    dictionary = corpora.Dictionary([i for i in lemmas])\n",
    "\n",
    "    # Convert dictionary into bag of words format: list of (token_id, token_count) tuples\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "    \n",
    "    # Calculate TFIDF based on bag of words counts for each token and return weights:\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    tfidf_weights=[]\n",
    "    \n",
    "    for doc in tfidf[bow_corpus]:\n",
    "        tfidf_weights.extend([[dictionary[ID], np.around(freq, decimals=2)] for ID, freq in doc])\n",
    "\n",
    "    # Sort TFIDF weights highest to lowest:\n",
    "    sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "    # Plot the top 10 weighted words:\n",
    "    top_10 = {k:v for k,v in sorted_tfidf_weights[:10]} # dictionary comprehension\n",
    "    plt.plot(list(top_10.keys()), list(top_10.values()))\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.title('Top 10 Lemmas (TFIDF) for ' + data_description);\n",
    "    \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
